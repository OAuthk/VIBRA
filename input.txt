Antigravityç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼šãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆ†é›¢ã¨Secretsã®å†çµ±åˆ
ã‚¿ã‚¹ã‚¯å:
Refactor CI/CD Pipelines for Strict Separation of Concerns and Secret Management
ã‚¿ã‚¹ã‚¯èª¬æ˜ (Instructions):
You are a Principal DevOps Architect. Your previous implementation was a good first draft, but it critically failed to adhere to our core architectural principle: strict separation of concerns between the data-fetching pipeline and the site-deployment pipeline.
Your new, urgent task is to refactor the entire CI/CD and Python script structure to enforce this separation, and to correctly re-integrate secret management.
You must treat the following two workflows as completely independent systems that only communicate via the cache-branch.
æŠ€è¡“ä»•æ§˜1ï¼šfetcher.yml ã¨ main.py ã®ä¿®æ­£ (ãƒ‡ãƒ¼ã‚¿å–å¾—ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³)
Objective:
This workflow's only responsibility is to fetch data and push a validated latest_trends.json to the cache-branch. It must not perform any site generation.
1.1. scripts/main.py ã®ä¿®æ­£:
Action: The script must be refactored to only perform the data pipeline tasks. The site generation logic must be completely removed.
Final Code for scripts/main.py:

import sys
import scraper
import analyzer
import enricher
import json
import os

def run_fetcher_pipeline():
    """Executes the complete data pipeline from scraping to saving cache files."""
    print("[INFO] Starting FETCHER pipeline...")
    
    # 1. Scrape
    raw_trends = scraper.fetch_raw_trends()
    if not raw_trends:
        print("[CRITICAL] No raw trends acquired. Halting.", file=sys.stderr)
        sys.exit(1)
    
    # 2. Analyze
    analyzed_trends = analyzer.analyze_trends(raw_trends)
    
    # 3. Enrich
    # This function must also generate 'scores_to_cache.json' internally
    enriched_trends = enricher.enrich_trends(analyzed_trends)
    
    # 4. Save final data to cache file
    output_dir = "cache"
    os.makedirs(output_dir, exist_ok=True)
    
    # This is the primary artifact of this workflow
    output_path = os.path.join(output_dir, "latest_trends.json")
    with open(output_path, 'w', encoding='utf-8') as f:
        # Note: We are not calling the to_dict_for_frontend() method here,
        # as the generator will handle that. We save the full enriched data.
        json.dump([item.to_dict() for item in enriched_trends], f, ensure_ascii=False, indent=2)
        
    print(f"[INFO] FETCHER pipeline complete. Saved data to {output_path}")

if __name__ == "__main__":
    run_fetcher_pipeline()

1.2. .github/workflows/fetcher.yml ã®ä¿®æ­£:
Action: Ensure this workflow correctly uses GitHub Actions Secrets and only executes the main.py script.
Final Code for .github/workflows/fetcher.yml:

name: Fetch and Cache Trend Data
on:
  schedule:
    - cron: '*/15 * * * *'
  workflow_dispatch:

jobs:
  fetch-and-cache:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install dependencies
        run: pip install -r requirements.txt

      - name: ğŸ“¥ Restore scores cache
        uses: actions/cache/restore@v4
        with:
          path: cache/previous_scores.json
          key: scores-cache-v1

      - name: ğŸ Run Data Pipeline
        env:
          MERCARI_AFFILIATE_ID: ${{ secrets.MERCARI_AFFILIATE_ID }}
          GA4_TRACKING_ID: ${{ secrets.GA4_TRACKING_ID }} # Pass all necessary secrets
        run: python scripts/main.py

      - name: ğŸ’¾ Save scores cache
        uses: actions/cache/save@v4
        with:
          path: cache/scores_to_cache.json
          key: scores-cache-v1-${{ github.run_id }}
      
      - name: ğŸš€ Commit and Push to cache-branch
        run: |
          git config --global user.name 'github-actions[bot]'
          git config --global user.email 'github-actions[bot]@users.noreply.github.com'
          git checkout --orphan cache-branch
          git rm -rf .
          git add cache/latest_trends.json cache/scores_to_cache.json
          git commit -m "chore: Update data cache"
          git push origin cache-branch --force

æŠ€è¡“ä»•æ§˜2ï¼šdeploy.yml ã¨ generator.py ã®ä¿®æ­£ (ã‚µã‚¤ãƒˆç”Ÿæˆãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³)
Objective:
This workflow's only responsibility is to fetch the cached data from cache-branch and generate the static site. It must not run any scraping or analysis.
2.1. scripts/generator.py ã®ä¿®æ­£:
Action: The script must be converted into a standalone, executable script that reads from the cache file.
Final Code for scripts/generator.py:

import json
import os
from jinja2 import Environment, FileSystemLoader
# ... other necessary imports like pytz, datetime

def generate_site_from_cache():
    """Reads data from the cache and generates all static site artifacts."""
    print("[INFO] Starting DEPLOYER pipeline...")
    
    # 1. Load data from cache
    cache_path = 'cache/latest_trends.json'
    try:
        with open(cache_path, 'r', encoding='utf-8') as f:
            trends_data = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError) as e:
        print(f"[CRITICAL] Failed to load cache file '{cache_path}'. Halting. Error: {e}", file=sys.stderr)
        sys.exit(1)

    # 2. Render templates (This is where to_dict_for_frontend() logic should live)
    # The generator should know how to transform the full data into the frontend format
    # For now, let's assume it's a direct pass-through
    
    # ... (Jinja2 rendering logic for index.html, sitemap.xml, OGP images etc.)
    # ... (This logic will read GA4_TRACKING_ID from environment variables)
    
    print("[INFO] DEPLOYER pipeline complete. Site generated in 'dist/'.")

if __name__ == "__main__":
    generate_site_from_cache()

2.2. .github/workflows/deploy.yml ã®ä¿®æ­£:
Action: The workflow must be simplified to only checkout the cache and run the generator.py script.
Final Code for .github/workflows/deploy.yml:

name: Build and Deploy Site
on:
  push:
    branches:
      - main
  schedule:
    - cron: '0 * * * *'
  workflow_dispatch:

permissions: # Required for GitHub Pages deployment
  contents: read
  pages: write
  id-token: write

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    env:
      GA4_TRACKING_ID: ${{ secrets.GA4_TRACKING_ID }}
    steps:
      - name: Checkout main branch (for templates and scripts)
        uses: actions/checkout@v4

      - name: â¬‡ï¸ Fetch latest data from cache-branch
        uses: actions/checkout@v4
        with:
          ref: cache-branch
          path: .cache-temp # Checkout to a temporary directory

      - name: Move cache file to expected location
        run: |
          mkdir -p cache
          mv .cache-temp/cache/latest_trends.json cache/latest_trends.json
      
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: pip install -r requirements.txt
        
      - name: ğŸ—ï¸ Generate Static Site
        run: python scripts/generator.py
        
      - name: ğŸš€ Deploy to GitHub Pages
        uses: actions/deploy-pages@v4

Final Instruction:
Your previous implementation mixed the concerns of these two pipelines. Please generate an implementation_plan.md to refactor the project according to these exact specifications, ensuring the fetcher and deployer are completely independent. I will not approve any plan that has main.py calling generator.py.

